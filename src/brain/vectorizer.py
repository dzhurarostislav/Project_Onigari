import logging
import re
import torch
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)

class VacancyVectorizer:
    def __init__(self, model_name: str = "BAAI/bge-m3"):
        # –ï—Å–ª–∏ –µ—Å—Ç—å GPU ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë, –∏–Ω–∞—á–µ CPU
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"üß† Loading {model_name} on {self.device}...")
        self.model = SentenceTransformer(
            model_name, 
            device=self.device, 
            model_kwargs={"torch_dtype": torch.float16}
        )

    def _clean_text(self, text: str) -> str:
        """–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∏—Å—Ç–∫–∞: —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –º—É—Å–æ—Ä"""
        if not text:
            return ""
        # –ó–∞–º–µ–Ω—è–µ–º –ª—é–±—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–±–µ–ª
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _prepare_input(self, vacancy) -> str:
        """–°–∫–ª–µ–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        title = vacancy.title or ""
        company = vacancy.company.name if vacancy.company else ""
        if vacancy.last_snapshot:
            desc = self._clean_text(vacancy.last_snapshot.full_description)
        else:
            desc = self._clean_text(vacancy.description)
        return f"–í–∞–∫–∞–Ω—Å–∏—è: {title}. –ö–æ–º–ø–∞–Ω–∏—è: {company}. –û–ø–∏—Å–∞–Ω–∏–µ: {desc}"

    async def process_vacancies(self, vacancies):
        """–ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π SQLAlchemy –≤ –≤–µ–∫—Ç–æ—Ä—ã"""
        if not vacancies:
            return []

        # 1. –ì–æ—Ç–æ–≤–∏–º —Ç–µ–∫—Å—Ç—ã
        texts = [self._prepare_input(v) for v in vacancies]
        
        # 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        # BGE-M3 —É–º–µ–µ—Ç –≤ dense, sparse –∏ multi-vector. –ù–∞–º –Ω—É–∂–µ–Ω dense (–æ–±—ã—á–Ω—ã–π –≤–µ–∫—Ç–æ—Ä).
        embeddings = self.model.encode(
            texts, 
            batch_size=16, 
            show_progress_bar=False,
            convert_to_numpy=True
        )

        # 3. –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ë–î
        return [
            {"b_id": v.id, "b_embedding": emb.tolist()} 
            for v, emb in zip(vacancies, embeddings)
        ]