import asyncio  # –î–ª—è –ø–∞—É–∑
import logging
import random

from database.models import VacancyStatus
from scrapers.schemas import VacancyBaseDTO

logger = logging.getLogger(__name__)


class DetailCrawler:
    def __init__(self, repo, scraper, parser) -> None:
        self.repo = repo
        self.scraper = scraper
        self.parser = parser

    async def crawl(self, limit: int = 10):
        logger.info(f"üëπ Starting deep crawl for {limit} vacancies...")

        # 1. –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∏–∑ –ë–î
        pending_vacancies = await self.repo.get_vacancies_by_status(VacancyStatus.NEW, limit)

        for vacancy in pending_vacancies:
            # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –ö–ê–ñ–î–£–Æ –∏—Ç–µ—Ä–∞—Ü–∏—é, —á—Ç–æ–±—ã –æ–¥–Ω–∞ –æ—à–∏–±–∫–∞ –Ω–µ —É–±–∏–ª–∞ –≤—Å—é –æ—Ö–æ—Ç—É
            try:
                # 2. –ú–∞–ø–∏–º –º–æ–¥–µ–ª—å –≤ DTO
                vacancy_dto = VacancyBaseDTO.model_validate(vacancy)

                # 3. –ö–∞—á–∞–µ–º HTML (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ await!)
                raw_html = await self.scraper.fetch_page_html(vacancy_dto.url)

                if not raw_html:
                    continue

                # 4. –í—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º "–º—è—Å–æ"
                vacancy_detail_dto = self.parser.parse_detail(raw_html, vacancy_dto)

                # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏ –º–µ–Ω—è–µ–º —Å—Ç–∞—Ç—É—Å –≤ –±–∞–∑–µ
                await self.repo.update_vacancy_details(vacancy.id, vacancy_detail_dto)

                logger.info(f"‚ú® Processed: {vacancy_dto.title}")

                # 6. –î–∞–µ–º —Å–∏—Å—Ç–µ–º–µ –≤—ã–¥–æ—Ö–Ω—É—Ç—å (–ø–∞—É–∑–∞ 2-3 —Å–µ–∫—É–Ω–¥—ã)
                await asyncio.sleep(random.uniform(2, 5))

            except Exception as e:
                logger.error(f"‚ùå Failed to process vacancy {vacancy.id}: {e}")
                continue  # –ò–¥–µ–º –∫ —Å–ª–µ–¥—É—é—â–µ–π –≤–∞–∫–∞–Ω—Å–∏–∏
