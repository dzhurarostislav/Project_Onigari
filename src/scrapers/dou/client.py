import logging
import os
from typing import Optional

from scrapers.base import BaseScraper
from scrapers.dou.parser import DouParser

logger = logging.getLogger(__name__)


class DouScraper(BaseScraper):
    def __init__(self):
        super().__init__(
            base_url="https://jobs.dou.ua/vacancies/",
            user_agent=os.getenv(
                "DOU_USER_AGENT",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            ),
            cookies_str=os.getenv("DOU_COOKIES", ""),
        )
        self.parser = DouParser()

    def _get_csrf_token(self) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç CSRF-—Ç–æ–∫–µ–Ω –∏–∑ –∫—É–∫–æ–≤ —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏."""
        token = self._session.cookies.get("csrftoken")
        if not token:
            logger.error("‚ùå CSRF token not found in cookies!")
            raise ValueError("Missing CSRF token")
        return token

    async def _fetch_more_via_ajax(self, category: str, count: int, csrf_token: str) -> dict:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç POST-–∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–¥–≥—Ä—É–∑–∫–∏ –Ω–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π."""
        url = f"{self.base_url}xhr-load/?category={category}"
        payload = {"csrfmiddlewaretoken": csrf_token, "count": count}

        headers = {
            "X-Requested-With": "XMLHttpRequest",
            "Referer": f"{self.base_url}?category={category}",
            "Origin": "https://jobs.dou.ua",
            "Accept": "application/json, text/javascript, */*; q=0.01",
        }

        try:
            logger.info(f"üëπ Onigari sending AJAX request with count={count}...")
            res = await self._session.post(url, data=payload, headers=headers)

            if res.status_code == 403:
                logger.error("‚ùå 403 Forbidden: DOU rejected the request.")
                return {}

            return res.json() if res.status_code == 200 else {}
        except Exception as e:
            logger.error(f"Error during AJAX load: {e}")
            return {}

    async def fetch_vacancies(self, category: str = "Python", **kwargs):
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ì–ï–ù–ï–†–ê–¢–û–†.
        –í–º–µ—Å—Ç–æ return list[...] –º—ã –¥–µ–ª–∞–µ–º yield list[...].
        """
        # –®–∞–≥ 1: –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (–≤—Å–µ–≥–¥–∞ –æ—Ç–¥–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å)
        main_url = f"{self.base_url}?category={category}"
        response = await self._session.get(main_url)

        if response.status_code == 200:
            first_batch = self.parser.parse_list(response.text)
            logger.info(f"‚ú® First page parsed: {len(first_batch)} vacancies")
            yield first_batch  # <--- –û—Ç–¥–∞–µ–º –ø–µ—Ä–≤—É—é –ø–∞—á–∫—É —Å—Ä–∞–∑—É
        else:
            return

        # –®–∞–≥ 2: AJAX —Ü–∏–∫–ª
        count = 20
        while True:
            try:
                await self._random_pause()

                # –¢–æ–∫–µ–Ω –º–æ–∂–µ—Ç –æ–±–Ω–æ–≤–∏—Ç—å—Å—è, –±–µ—Ä–µ–º —Å–≤–µ–∂–∏–π
                current_token = self._get_csrf_token()

                # –ó–∞–ø—Ä–æ—Å
                data = await self._fetch_more_via_ajax(category, count, current_token)

                if not data or not data.get("html"):
                    logger.info("üí® Response is empty or no HTML.")
                    break

                # –ü–∞—Ä—Å–∏–Ω–≥
                new_batch = self.parser.parse_list(data.get("html", ""))
                if not new_batch:
                    break

                logger.info(f"‚ú® Yielding batch of {len(new_batch)} items (offset {count})")
                yield new_batch  # <--- –û—Ç–¥–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é –ø–∞—á–∫—É

                if data.get("last") is True:
                    logger.info("üèÅ Server said: last=true.")
                    break

                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –õ–û–ì–ò–ö–ò:
                # –°–µ—Ä–≤–µ—Ä –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–µ 'num', –∫–æ—Ç–æ—Ä–æ–µ –≥–æ–≤–æ—Ä–∏—Ç, —Å–∫–æ–ª—å–∫–æ –æ–Ω –æ—Ç–¥–∞–ª.
                # –û–±—ã—á–Ω–æ —ç—Ç–æ 40. –ú—ã –¥–æ–ª–∂–Ω—ã —à–∞–≥–∞—Ç—å –Ω–∞ —ç—Ç–æ —á–∏—Å–ª–æ, —á—Ç–æ–±—ã –Ω–µ —Ç–æ–ø—Ç–∞—Ç—å—Å—è –Ω–∞ –º–µ—Å—Ç–µ.
                step = data.get("num", 40)
                count += step

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è AJAX cycle interrupted: {e}")
                break

    async def fetch_page_html(self, url: str) -> Optional[str]:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è HTML.
        –û—Ç–≤–µ—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∑–∞ —Å–µ—Ç—å: –∑–∞–≥–æ–ª–æ–≤–∫–∏, –∫—É–∫–∏, –æ–±—Ö–æ–¥ –∑–∞—â–∏—Ç—ã.
        """
        try:
            safe_url = str(url)
            logger.info(f"üì° Hunting for content at: {url}")
            # –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ —Å–µ—Å—Å–∏—é —Å —Ç–µ–º–∏ –∂–µ –∫—É–∫–∞–º–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
            response = await self._session.get(safe_url)

            if response.status_code == 200:
                return response.text

            logger.error(f"‚ùå Page fetch failed: {response.status_code} for {url}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Network error during hunt: {e}")
            return None
