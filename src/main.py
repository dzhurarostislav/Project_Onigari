import asyncio
import logging
import sys

from sqlalchemy import text

from database.models import Base
from database.service import VacancyRepository
from database.sessions import async_session, engine
from scrapers.crawler import DetailCrawler
from scrapers.dou.client import DouScraper
from scrapers.dou.parser import DouParser


# 1. –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–æ–≤
def setup_logging(level=logging.INFO):
    log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    logging.basicConfig(level=level, format=log_format, handlers=[logging.StreamHandler(sys.stdout)])
    # –¢–∏—Ö–∏–π —Ä–µ–∂–∏–º –¥–ª—è —à—É–º–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
    logging.getLogger("sqlalchemy.engine").setLevel(logging.WARNING)
    logging.getLogger("curl_cffi").setLevel(logging.WARNING)


logger = logging.getLogger("onigari.main")


async def setup_database():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã: —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∏ —Ç–∞–±–ª–∏—Ü—ã"""
    try:
        async with engine.begin() as conn:
            await conn.execute(text("CREATE EXTENSION IF NOT EXISTS vector"))
            logger.info("‚úÖ PGVector extension is ready")

            # –í–ù–ò–ú–ê–ù–ò–ï: –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Alembic, –Ω–æ –¥–ª—è —Å—Ç–∞—Ä—Ç–∞ ‚Äî –æ–∫
            await conn.run_sync(Base.metadata.create_all)
            logger.info("‚úÖ Database tables verified")
    except Exception as e:
        logger.error(f"‚ùå Database setup failed: {e}")
        raise


async def run_scrapers():
    """–¶–∏–∫–ª —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤–Ω–µ—à–Ω–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    async with async_session() as session:
        repository = VacancyRepository(session)

        async with DouScraper() as scraper:
            logger.info("üì° Scanning DOU for new opportunities...")
            # –ú–æ–∂–Ω–æ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            async for batch in scraper.fetch_vacancies(category="Python"):
                if not batch:
                    continue

                added_count = await repository.batch_upsert(batch)
                if added_count > 0:
                    logger.info(f"üëπ Trapped {added_count} new demons in the database.")


async def run_deep_extraction():
    """–§–∞–∑–∞ 2: –ì–ª—É–±–æ–∫–æ–µ –ø–æ—Ç—Ä–æ—à–µ–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–π (Extraction)"""
    async with async_session() as session:
        repository = VacancyRepository(session)
        # –ù–∞–º –Ω—É–∂–Ω—ã "—Ä—É–∫–∏" –∏ "–≥–ª–∞–∑–∞" –¥–ª—è –∫—Ä–∞–≤–ª–µ—Ä–∞
        async with DouScraper() as scraper:
            parser = DouParser()

            crawler = DetailCrawler(repository, scraper, parser)

            logger.info("üî™ Starting deep extraction of vacancy details...")
            # –ë–µ—Ä–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, 20 —à—Ç—É–∫ –∑–∞ —Ä–∞–∑
            await crawler.crawl(limit=20)


async def main():
    setup_logging()  # –í—ã–∑—ã–≤–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ª–æ–≥–æ–≤ –ø–µ—Ä–≤—ã–º –¥–µ–ª–æ–º
    logger.info("üëπ Project Onigari (È¨ºÁã©„Çä) is waking up...")

    await setup_database()

    while True:
        try:
            logger.info("üöÄ Phase 1: Discovery started...")
            await run_scrapers()
            logger.info("üöÄ Phase 2: Deep Extraction started...")
            await run_deep_extraction()
            logger.info("üèÅ Full hunting cycle completed successfully.")
        except Exception as e:
            # exc_info=True –≤—ã–≤–µ–¥–µ—Ç –≤–µ—Å—å traceback –æ—à–∏–±–∫–∏
            logger.error(f"‚ö†Ô∏è Scraper cycle failed: {e}", exc_info=True)

        logger.info("üí§ Sleeping for 1 hour before next hunt...")
        await asyncio.sleep(60 * 60)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("üëã Onigari is going to sleep (KeyboardInterrupt)")
    except Exception as e:
        logger.critical(f"üí• Fatal crash: {e}")
        sys.exit(1)
